\chapter{Conclusion and Outlook}
\label{cha:conclusion_and_outlook}
% Final, short summary of your work.
In this work, we explore the applicability of the serverless platform as an additional in-memory caching layer. We propose a novel reactive in-memory caching system that relies on the primary Redis-based caching layer, similar to other managed cloud services like ElastiCache. By leveraging the memory capacity of serverless functions, our system can improve the responsiveness of a reactive design without over-provisioning in-memory caching resources. Compared to the billing granularity of node hours of current managed in-memory caching services, our system can provide fine-grained control and therefore provides better elasticity to build a cost-effective in-memory caching system. The reactive design forms the basis for a fully managed service that frees system administrators from the burden of managing the in-memory cache themselves in a cost-effective manner, depending on the workload.

~\\
The main contribution of our work is the reactive design of our orchestrator, which is responsible for managing the in-memory caching layers and the integration of the serverless platform as one of these layers. Our system is deployed on AWS infrastructure and uses EC2 units to host our Redis-based layer, while AWS Lambda is the serverless platform used in our system. The reactive design enables our orchestrator to manage the in-memory caching layers without over-provisioning caching resources cost-effectively. Our orchestrator can provision in-memory caching capacity within milliseconds using the serverless platform. Afterward, the system switches to the less expensive Redis-based layer when the load persists. However, starting the Redis layer takes some time; the intermediate time can also be covered with the serverless platform.

~\\
Cost efficiency and performance guarantees are highly dependent on the workload, as the reactive design acts based on this load. Simulations are used to evaluate and study the usability and cost of our system for different workloads. The current design of our system clearly shows some limitations for general workloads, as the reactive mechanism is designed to handle burst workloads. We target this setting because the current managed services are not cost-effective due to their design, while a self-managed system is challenging to develop for the unknown arrival of these burst workloads. The results show that our reactive system achieves a cache hit rate of about 95\% in this setting. In the case of bursty workloads, our system can reduce the cost of an always running caching system and achieves similar or even lower costs than a self-hosted Redis instance running only for the duration of the workload. The evaluation clearly shows the potential of our reactive design and serverless computing in such a system. 

\newpage
\noindent
Although our work targets whether the serverless platform is useful for building a reactive managed system, the implementation still requires future work to overcome current limitations. In addition, we propose some interesting directions for future work:
\begin{itemize}
    \item \textbf{Proactive Endpoint Scheduler:} Incorporating a proactive endpoint scheduler could help our system handle workloads where a reactive design leads to poor performance. A proactive endpoint scheduler would require prediction, a separate research topic.
    \item \textbf{InfiniCache:} InfiniCache~\cite{wang_infinicache_2020} provides a novel approach to in-memory caching as a request-based pay-per-use model. Integrating their approach for infrequent requests could help our system become a 100\% in-memory caching system.
    \item \textbf{Client Library:} Suppose the client is running inside the AWS infrastructure. The additional copy on the reverse proxy can be avoided by including the reverse proxy in a client library running on the same host as the client, similar to InfiniCache~\cite{wang_infinicache_2020}.
\end{itemize}

% Our work provides a solid foundation for future work on extending our system to compete with a managed cloud service such as ElastiCache. Our work builds a solid foundation for future work to extend our system to compete with a managed cloud service like ElastiCache.

% \section{Improve Endpoint Scheduler}
% \label{sec:improve_endpoint_scheduler}
% The improvement of the endpoint scheduler offers many exciting starting points for future work. The reactive design of our endpoint scheduler could be improved by using proactive decisions. Integrating a proactive endpoint scheduler could help our system handle workloads where a reactive design leads to poor performance. However, integrating a proactive endpoint scheduler would require some prediction, which is not straightforward and opens a new research topic on its own. However, it would open many new possibilities for designing the endpoint scheduler.

% ~\\
% A more promising approach is offered by InfiniCache~\cite{wang_infinicache_2020} to provide caching for infrequent requests. Compared to our system, they do not use the serverless platform as a server listening to requests. Instead, their request directly invokes the function which stores the object. Using their system to manage the AWS Lambda execution environments to keep them warm allows for fast startup times and fast data retrieval. Extending our system by incorporating their approach to handling requests currently forwarded to S3 could improve average latency by achieving 100\% in-memory caching. The startup latencies measured in Section~\ref{subsec:startup_times} look promising for warm Lambda starts, and compared to these measurements, the reverse proxy would call the function itself, further reducing startup latency by eliminating the additional delay in message passing. Thus, integrating their approach could lead to a 100 percent in-memory caching system that can handle workloads that our system cannot respond to cost-efficiently. This adaption would also remove the direct impact of the system parameters concerning the in-memory portion.

% \section{Reverse Proxy as Client Library}
% \label{sec:proxy_as_client_library}
% Section~\ref{subsec:end_to_end_latency} has shown that the way we test and deploy our system is not ideal in the context of in-memory caching. Internet traffic latency accounts for a large portion of the end-to-end latency and drastically impacts the potential speedup. Deploying the reverse proxy on a separate EC2 instance requires an additional copy of the data, impacting end-to-end latency as object size increases. As long as the client runs outside the AWS infrastructure, we can only improve the simple copy. However, suppose the client is running on the AWS infrastructure. In that case, our reverse proxy could be wrapped in a client library that provides the same \code{get} API but runs on the same host as the client, similar to the design of InfiniCache~\cite{wang_infinicache_2020}. By customizing our system in this way, we can avoid the additional copy, resulting in better end-to-end latency for each layer, highlighting the latency difference between in-memory caching and the persistent storage layer. 